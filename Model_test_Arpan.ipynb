{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e6cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = data_train['full_text']\n",
    "cohesion = data_train['cohesion']\n",
    "syntax = data_train['syntax']\n",
    "vocabulary = data_train['vocabulary']\n",
    "phraseology = data_train['phraseology']\n",
    "grammar = data_train['grammar']\n",
    "conventions = data_train['conventions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60adabdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/arpan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349cf927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanText(sentences):\n",
    "    sentences = sentences.apply(lambda sequence : \n",
    "                                [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n",
    "    sentences = sentences.apply(lambda wrd: ''.join(wrd))\n",
    "    sentences = sentences.apply(lambda sequence: \n",
    "                                [word for word in sequence.split() if word not in stop_words])\n",
    "    sentences = sentences.apply(lambda wrd: ' '.join(wrd))\n",
    "    return sentences\n",
    "\n",
    "def CleanFeatures(sentences):\n",
    "    sentences = sentences.apply(lambda sequence:\n",
    "                                            [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n",
    "    sentences = sentences.apply(lambda wrd: ''.join(wrd))\n",
    "    sentences = sentences.apply(lambda sequence:\n",
    "                                            [word for word in sequence.split() if word not in stop_words])\n",
    "    sentences = sentences.apply(lambda wrd: ' '.join(wrd))\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72d2082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       think students would benefit learning homebeca...\n",
       "1       problem change let best matter happening chang...\n",
       "2       dear principal u change school policy grade b ...\n",
       "3       best time life become agree greatest accomplis...\n",
       "4       small act kindness impact people change people...\n",
       "                              ...                        \n",
       "3906    believe using cellphones class education us go...\n",
       "3907    working alone students argue decission proyect...\n",
       "3908    problem chance best think quote cant best ever...\n",
       "3909    many people disagree albert schweitzers quote ...\n",
       "3910    think failure main thing people consist goals ...\n",
       "Name: full_text, Length: 3911, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = CleanFeatures(full_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d24ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e42b60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_words = [len(text.split()) for text in clean_text]\n",
    "#seq_len = max(list_words)\n",
    "seq_len = 512\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8267410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f67c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_mask = []\n",
    "for text in clean_text:\n",
    "    tokens = tokenizer.encode_plus(text, max_length=seq_len, padding='max_length',\n",
    "                         truncation=True, return_token_type_ids= True,\n",
    "                         return_tensors = 'np')\n",
    "    input_ids.append(tokens['input_ids'])\n",
    "    attention_mask.append(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59289a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3911, 1, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = np.asarray(input_ids)\n",
    "attention_mask = np.asarray(attention_mask)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e23e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 512)\n",
      "(3911, 512)\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.reshape(input_ids, (input_ids.shape[0], input_ids.shape[2]))\n",
    "attention_mask = np.reshape(attention_mask, (attention_mask.shape[0], attention_mask.shape[2]))\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7bed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax = np.asarray(syntax)\n",
    "cohesion = np.asarray(cohesion)\n",
    "vocabulary = np.asarray(vocabulary)\n",
    "phraseology = np.asarray(phraseology)\n",
    "grammar = np.asarray(grammar)\n",
    "conventions = np.asarray(conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2205430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "from keras.layers import LSTM, Lambda\n",
    "from keras.layers import add, maximum, subtract, minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8736aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(inputs, units):\n",
    "    x = tf.keras.layers.LSTM(units,return_sequences = True, dropout=0.15)(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a72371de",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 32\n",
    "input_ids_m = tf.keras.layers.Input(shape = (seq_len, ), dtype = 'int32')\n",
    "attention_mask_n = tf.keras.layers.Input(shape = (seq_len, ),  dtype = 'int32')\n",
    "bert_m = bert(input_ids_m, attention_mask = attention_mask_n)[0]\n",
    "x = resnet(bert_m, units)\n",
    "for stack in range(3):\n",
    "    for block in range(1):\n",
    "        y = resnet(x, units)\n",
    "        if stack > 0 and block == 0:\n",
    "            x = tf.keras.layers.LSTM(units, return_sequences = True, dropout=0.15, recurrent_dropout=0.15)(x)\n",
    "        x = minimum([x, y])\n",
    "    units *=2\n",
    "x1 = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x2 = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "x = tf.keras.layers.concatenate([x1, x2], name=\"our_param\")\n",
    "y = tf.keras.layers.Dense(1,  name='syntax')(x)\n",
    "y2 = tf.keras.layers.Dense(1, name='cohesion')(x)\n",
    "y3 = tf.keras.layers.Dense(1, name='vocabulary')(x)\n",
    "test_model = tf.keras.models.Model(inputs = [input_ids_m, attention_mask_n], outputs = [y, y2, y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e40086",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8f05acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_weights('bert_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57863722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 512, 32)      102528      ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512, 32)      8320        ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " minimum (Minimum)              (None, 512, 32)      0           ['lstm[0][0]',                   \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 512, 64)      24832       ['minimum[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 512, 64)      24832       ['minimum[0][0]']                \n",
      "                                                                                                  \n",
      " minimum_1 (Minimum)            (None, 512, 64)      0           ['lstm_3[0][0]',                 \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 512, 128)     98816       ['minimum_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 512, 128)     98816       ['minimum_1[0][0]']              \n",
      "                                                                                                  \n",
      " minimum_2 (Minimum)            (None, 512, 128)     0           ['lstm_5[0][0]',                 \n",
      "                                                                  'lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['minimum_2[0][0]']              \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 128)         0           ['minimum_2[0][0]']              \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " our_param (Concatenate)        (None, 256)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'global_max_pooling1d[0][0]']  \n",
      "                                                                                                  \n",
      " syntax (Dense)                 (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      " cohesion (Dense)               (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      " vocabulary (Dense)             (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,669,187\n",
      "Trainable params: 358,915\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c78fd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_model.input\n",
    "x = test_model.get_layer('our_param')(inputs)\n",
    "outputs = tf.keras.layers.Dense(6)(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "094428a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer= \"adam\")\n",
    "model.layers[0].trainable = False\n",
    "model.layers[1].trainable = False\n",
    "model.layers[2].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "def6905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " our_param (Concatenate)        multiple             0           ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6)            6150        ['our_param[1][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,150\n",
      "Trainable params: 6,150\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_size = 3300\n",
    "x_train = [input_ids[:train_size], attention_mask[:train_size]]\n",
    "y_train = [syntax[:train_size], vocabulary[:train_size], cohesion[:train_size], \n",
    "           phraseology[:train_size], grammar[:train_size], conventions[:train_size]]\n",
    "x_test = [input_ids[train_size:], attention_mask[train_size:]]\n",
    "y_test = [syntax[train_size:], vocabulary[train_size:], cohesion[train_size:], \n",
    "           phraseology[train_size:], grammar[train_size:], conventions[train_size:]]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c20fdfa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 6) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/dx/sc89948n7r9d_y3ms23yc8c40000gn/T/__autograph_generated_file042ef9c4.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/miniconda3/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 6) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9abe5ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3911, 512)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0df0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
