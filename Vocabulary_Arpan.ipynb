{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d1de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49872422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a5746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa368f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3911.000000</td>\n",
       "      <td>3911.000000</td>\n",
       "      <td>3911.000000</td>\n",
       "      <td>3911.000000</td>\n",
       "      <td>3911.000000</td>\n",
       "      <td>3911.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.127077</td>\n",
       "      <td>3.028254</td>\n",
       "      <td>3.235745</td>\n",
       "      <td>3.116850</td>\n",
       "      <td>3.032856</td>\n",
       "      <td>3.081053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.662542</td>\n",
       "      <td>0.644399</td>\n",
       "      <td>0.583148</td>\n",
       "      <td>0.655997</td>\n",
       "      <td>0.699841</td>\n",
       "      <td>0.671450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cohesion       syntax   vocabulary  phraseology      grammar  \\\n",
       "count  3911.000000  3911.000000  3911.000000  3911.000000  3911.000000   \n",
       "mean      3.127077     3.028254     3.235745     3.116850     3.032856   \n",
       "std       0.662542     0.644399     0.583148     0.655997     0.699841   \n",
       "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "25%       2.500000     2.500000     3.000000     2.500000     2.500000   \n",
       "50%       3.000000     3.000000     3.000000     3.000000     3.000000   \n",
       "75%       3.500000     3.500000     3.500000     3.500000     3.500000   \n",
       "max       5.000000     5.000000     5.000000     5.000000     5.000000   \n",
       "\n",
       "       conventions  \n",
       "count  3911.000000  \n",
       "mean      3.081053  \n",
       "std       0.671450  \n",
       "min       1.000000  \n",
       "25%       2.500000  \n",
       "50%       3.000000  \n",
       "75%       3.500000  \n",
       "max       5.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a423220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = data_train['full_text']\n",
    "cohesion = data_train['cohesion']\n",
    "syntax = data_train['syntax']\n",
    "vocabulary = data_train['vocabulary']\n",
    "phraseology = data_train['phraseology']\n",
    "grammar = data_train['grammar']\n",
    "conventions = data_train['conventions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fe0b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/arpan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c98219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanText(sentences):\n",
    "    sentences = sentences.apply(lambda sequence : \n",
    "                                [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n",
    "    sentences = sentences.apply(lambda wrd: ''.join(wrd))\n",
    "    sentences = sentences.apply(lambda sequence: \n",
    "                                [word for word in sequence.split() if word not in stop_words])\n",
    "    sentences = sentences.apply(lambda wrd: ' '.join(wrd))\n",
    "    return sentences\n",
    "\n",
    "def CleanFeatures(sentences):\n",
    "    sentences = sentences.apply(lambda sequence:\n",
    "                                            [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n",
    "    sentences = sentences.apply(lambda wrd: ''.join(wrd))\n",
    "    sentences = sentences.apply(lambda sequence:\n",
    "                                            [word for word in sequence.split() if word not in stop_words])\n",
    "    sentences = sentences.apply(lambda wrd: ' '.join(wrd))\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af213cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       think students would benefit learning homebeca...\n",
       "1       problem change let best matter happening chang...\n",
       "2       dear principal u change school policy grade b ...\n",
       "3       best time life become agree greatest accomplis...\n",
       "4       small act kindness impact people change people...\n",
       "                              ...                        \n",
       "3906    believe using cellphones class education us go...\n",
       "3907    working alone students argue decission proyect...\n",
       "3908    problem chance best think quote cant best ever...\n",
       "3909    many people disagree albert schweitzers quote ...\n",
       "3910    think failure main thing people consist goals ...\n",
       "Name: full_text, Length: 3911, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = CleanFeatures(full_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852fafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e43725f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_words = [len(text.split()) for text in clean_text]\n",
    "#seq_len = max(list_words)\n",
    "seq_len = 512\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade24acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bd9d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_mask = []\n",
    "for text in clean_text:\n",
    "    tokens = tokenizer.encode_plus(text, max_length=seq_len, padding='max_length',\n",
    "                         truncation=True, return_token_type_ids= True,\n",
    "                         return_tensors = 'np')\n",
    "    input_ids.append(tokens['input_ids'])\n",
    "    attention_mask.append(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23dc60d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3911, 1, 512)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = np.asarray(input_ids)\n",
    "attention_mask = np.asarray(attention_mask)\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fadb47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 512)\n",
      "(3911, 512)\n"
     ]
    }
   ],
   "source": [
    "input_ids = np.reshape(input_ids, (input_ids.shape[0], input_ids.shape[2]))\n",
    "attention_mask = np.reshape(attention_mask, (attention_mask.shape[0], attention_mask.shape[2]))\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806c2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax = np.asarray(syntax)\n",
    "cohesion = np.asarray(cohesion)\n",
    "vocabulary = np.asarray(vocabulary)\n",
    "phraseology = np.asarray(phraseology)\n",
    "grammar = np.asarray(grammar)\n",
    "conventions = np.asarray(conventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e59bfb",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61e5c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7cc781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Lambda\n",
    "from keras.layers import add, maximum, subtract, minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d7532e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(inputs, units):\n",
    "    x = tf.keras.layers.LSTM(units,return_sequences = True, dropout=0.15)(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34313c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 32\n",
    "input_ids_m = tf.keras.layers.Input(shape = (seq_len, ), dtype = 'int32')\n",
    "attention_mask_n = tf.keras.layers.Input(shape = (seq_len, ),  dtype = 'int32')\n",
    "bert_m = bert(input_ids_m, attention_mask = attention_mask_n)[0]\n",
    "x = resnet(bert_m, units)\n",
    "for stack in range(3):\n",
    "    for block in range(1):\n",
    "        y = resnet(x, units)\n",
    "        if stack > 0 and block == 0:\n",
    "            x = tf.keras.layers.LSTM(units, return_sequences = True, dropout=0.15, recurrent_dropout=0.15)(x)\n",
    "        x = minimum([x, y])\n",
    "    units *=2\n",
    "x1 = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x2 = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "x = tf.keras.layers.concatenate([x1, x2], name=\"our_param\")\n",
    "y = tf.keras.layers.Dense(1,  name='syntax')(x)\n",
    "y2 = tf.keras.layers.Dense(1, name='cohesion')(x)\n",
    "y3 = tf.keras.layers.Dense(1, name='vocabulary')(x)\n",
    "m = tf.keras.models.Model(inputs = [input_ids_m, attention_mask_n], outputs = [y, y2, y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c35f62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16b684b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_3[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_4[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (None, 512, 32)      102528      ['tf_bert_model[1][0]']          \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (None, 512, 32)      8320        ['lstm_6[0][0]']                 \n",
      "                                                                                                  \n",
      " minimum_3 (Minimum)            (None, 512, 32)      0           ['lstm_6[0][0]',                 \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)                  (None, 512, 64)      24832       ['minimum_3[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)                  (None, 512, 64)      24832       ['minimum_3[0][0]']              \n",
      "                                                                                                  \n",
      " minimum_4 (Minimum)            (None, 512, 64)      0           ['lstm_9[0][0]',                 \n",
      "                                                                  'lstm_8[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_11 (LSTM)                 (None, 512, 128)     98816       ['minimum_4[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_10 (LSTM)                 (None, 512, 128)     98816       ['minimum_4[0][0]']              \n",
      "                                                                                                  \n",
      " minimum_5 (Minimum)            (None, 512, 128)     0           ['lstm_11[0][0]',                \n",
      "                                                                  'lstm_10[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['minimum_5[0][0]']              \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 128)         0           ['minimum_5[0][0]']              \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " our_param (Concatenate)        (None, 256)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " syntax (Dense)                 (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      " cohesion (Dense)               (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      " vocabulary (Dense)             (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,669,187\n",
      "Trainable params: 358,915\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3944140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(m, show_shapes=True, \n",
    "                          show_dtype=False, \n",
    "                          show_layer_names=True, \n",
    "                          expand_nested=True,\n",
    "                          show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38f7c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss=\"mse\", optimizer= \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373145d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68d0ef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 17:28:04.981457: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 2031s 9s/step - loss: 1.8195 - syntax_loss: 0.5952 - cohesion_loss: 0.6763 - vocabulary_loss: 0.5481 - val_loss: 1.0873 - val_syntax_loss: 0.3896 - val_cohesion_loss: 0.3870 - val_vocabulary_loss: 0.3108\n",
      "Epoch 2/6\n",
      "233/233 [==============================] - 2074s 9s/step - loss: 1.1234 - syntax_loss: 0.3927 - cohesion_loss: 0.4111 - vocabulary_loss: 0.3196 - val_loss: 0.8377 - val_syntax_loss: 0.2828 - val_cohesion_loss: 0.3225 - val_vocabulary_loss: 0.2324\n",
      "Epoch 3/6\n",
      "233/233 [==============================] - 2065s 9s/step - loss: 0.9531 - syntax_loss: 0.3355 - cohesion_loss: 0.3531 - vocabulary_loss: 0.2645 - val_loss: 0.7624 - val_syntax_loss: 0.2809 - val_cohesion_loss: 0.2831 - val_vocabulary_loss: 0.1984\n",
      "Epoch 4/6\n",
      "233/233 [==============================] - 2083s 9s/step - loss: 0.9046 - syntax_loss: 0.3198 - cohesion_loss: 0.3320 - vocabulary_loss: 0.2527 - val_loss: 0.6933 - val_syntax_loss: 0.2444 - val_cohesion_loss: 0.2674 - val_vocabulary_loss: 0.1815\n",
      "Epoch 5/6\n",
      "233/233 [==============================] - 2095s 9s/step - loss: 0.8805 - syntax_loss: 0.3143 - cohesion_loss: 0.3228 - vocabulary_loss: 0.2434 - val_loss: 0.6975 - val_syntax_loss: 0.2537 - val_cohesion_loss: 0.2587 - val_vocabulary_loss: 0.1850\n",
      "Epoch 6/6\n",
      "233/233 [==============================] - 4522s 19s/step - loss: 0.8737 - syntax_loss: 0.3078 - cohesion_loss: 0.3240 - vocabulary_loss: 0.2419 - val_loss: 0.9415 - val_syntax_loss: 0.3316 - val_cohesion_loss: 0.3319 - val_vocabulary_loss: 0.2780\n"
     ]
    }
   ],
   "source": [
    "history_training = m.fit([input_ids, attention_mask],\n",
    "                         y = [syntax, cohesion, vocabulary],\n",
    "                         batch_size= 16,\n",
    "                         validation_split = 0.05, \n",
    "                         epochs= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9fab488",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('bert_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb0b1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_weights('bert_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c616e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 32\n",
    "input_ids_m = tf.keras.layers.Input(shape = (seq_len, ), dtype = 'int32')\n",
    "attention_mask_n = tf.keras.layers.Input(shape = (seq_len, ),  dtype = 'int32')\n",
    "bert_m = bert(input_ids_m, attention_mask = attention_mask_n)[0]\n",
    "x = resnet(bert_m, units)\n",
    "for stack in range(3):\n",
    "    for block in range(1):\n",
    "        y = resnet(x, units)\n",
    "        if stack > 0 and block == 0:\n",
    "            x = tf.keras.layers.LSTM(units, return_sequences = True, dropout=0.15, recurrent_dropout=0.15)(x)\n",
    "        x = minimum([x, y])\n",
    "    units *=2\n",
    "x1 = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x2 = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "x = tf.keras.layers.concatenate([x1, x2], name=\"our_param\")\n",
    "y = tf.keras.layers.Dense(1,  name='syntax')(x)\n",
    "y2 = tf.keras.layers.Dense(1, name='cohesion')(x)\n",
    "y3 = tf.keras.layers.Dense(1, name='vocabulary')(x)\n",
    "test_model = tf.keras.models.Model(inputs = [input_ids_m, attention_mask_n], outputs = [y, y2, y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a04f1359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_5[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_6[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)                 (None, 512, 32)      102528      ['tf_bert_model[2][0]']          \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)                 (None, 512, 32)      8320        ['lstm_12[0][0]']                \n",
      "                                                                                                  \n",
      " minimum_6 (Minimum)            (None, 512, 32)      0           ['lstm_12[0][0]',                \n",
      "                                                                  'lstm_13[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_15 (LSTM)                 (None, 512, 64)      24832       ['minimum_6[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 (None, 512, 64)      24832       ['minimum_6[0][0]']              \n",
      "                                                                                                  \n",
      " minimum_7 (Minimum)            (None, 512, 64)      0           ['lstm_15[0][0]',                \n",
      "                                                                  'lstm_14[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)                 (None, 512, 128)     98816       ['minimum_7[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 (None, 512, 128)     98816       ['minimum_7[0][0]']              \n",
      "                                                                                                  \n",
      " minimum_8 (Minimum)            (None, 512, 128)     0           ['lstm_17[0][0]',                \n",
      "                                                                  'lstm_16[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 128)         0           ['minimum_8[0][0]']              \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 128)         0           ['minimum_8[0][0]']              \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " our_param (Concatenate)        (None, 256)          0           ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " syntax (Dense)                 (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      " cohesion (Dense)               (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      " vocabulary (Dense)             (None, 1)            257         ['our_param[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,669,187\n",
      "Trainable params: 358,915\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3788809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_weights('bert_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd4d81ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x291a35d30>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f5b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
